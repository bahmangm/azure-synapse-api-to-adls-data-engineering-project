{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import date, timedelta\n",
        "\n",
        "from pyspark.sql.functions import when, col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Ensure the below library is installed on your cluster\n",
        "import reverse_geocoder as rg"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the JSON string\n",
        "bronze_data = json.loads(bronze_output)\n",
        "\n",
        "# Access individual variables\n",
        "start_date = bronze_data.get(\"start_date\", \"\")\n",
        "silver_adls = bronze_data.get(\"silver_adls\", \"\")\n",
        "gold_adls = bronze_data.get(\"gold_adls\", \"\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.parquet(silver_data).filter(col('time') > start_date)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.limit(10) # added to speed up processings as during testing it was proving a bottleneck\n",
        "# The problem is caused by the Python UDF (reverse_geocoder) being a bottleneck due to its non-parallel nature and high computational cost per task"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def get_country_code(lat, lon):\n",
        "    \"\"\"\n",
        "    Retrieve the country code for a given latitude and longitude.\n",
        "\n",
        "    Parameters:\n",
        "    lat (float or str): Latitude of the location.\n",
        "    lon (float or str): Longitude of the location.\n",
        "\n",
        "    Returns:\n",
        "    str: Country code of the location, retrieved using the reverse geocoding API.\n",
        "\n",
        "    Example:\n",
        "    >>> get_country_details(48.8588443, 2.2943506)\n",
        "    'FR'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        coordinates = (float(lat), float(lon))\n",
        "        result = rg.search(coordinates)[0].get('cc')\n",
        "        print(f\"Processed coordinates: {coordinates} -> {result}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing coordinates: {lat}, {lon} -> {str(e)}\")\n",
        "        return None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# registering the udfs so they can be used on spark dataframes\n",
        "get_country_code_udf = udf(get_country_code, StringType())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get_country_code(48.8588443, 2.2943506)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# adding country_code and city attributes\n",
        "df_with_location = \\\n",
        "                df.\\\n",
        "                    withColumn(\"country_code\", get_country_code_udf(col(\"latitude\"), col(\"longitude\")))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_location.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# adding significance classification\n",
        "df_with_location_sig_class = \\\n",
        "                            df_with_location.\\\n",
        "                                withColumn('sig_class', \n",
        "                                            when(col(\"sig\") < 100, \"Low\").\\\n",
        "                                            when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
        "                                            otherwise(\"High\")\n",
        "                                            )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_location_sig_class.printSchema()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the transformed DataFrame to the Silver container\n",
        "gold_output_path = f\"{gold_adls}earthquake_events_gold/\"\n",
        "\n",
        "# Append DataFrame to Silver container in Parquet format\n",
        "df_with_location_sig_class.write.mode('append').parquet(gold_output_path)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}