{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "synapsesApache",
              "statement_id": 9,
              "statement_ids": [
                9
              ],
              "state": "finished",
              "livy_statement_state": "available",
              "spark_jobs": null,
              "session_id": "5",
              "normalized_state": "finished",
              "queued_time": "2025-04-04T18:01:49.8582253Z",
              "session_start_time": null,
              "execution_start_time": "2025-04-04T18:01:49.8594596Z",
              "execution_finish_time": "2025-04-04T18:01:50.0954707Z",
              "parent_msg_id": "1c0cf09c-cbdf-463c-9bc6-ae9d0b21646e"
            },
            "text/plain": "StatementMeta(synapsesApache, 5, 9, Finished, Available, Finished)"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "import json\n",
        "from datetime import date, timedelta\n",
        "\n",
        "from pyspark.sql.functions import when, col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# Ensure the below library is installed on your cluster\n",
        "import reverse_geocoder as rg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Parse the JSON string\n",
        "bronze_data = json.loads(bronze_output)\n",
        "\n",
        "# Access individual variables\n",
        "start_date = bronze_data.get(\"start_date\", \"\")\n",
        "silver_adls = bronze_data.get(\"silver_adls\", \"\")\n",
        "gold_adls = bronze_data.get(\"gold_adls\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = spark.read.parquet(silver_data).filter(col('time') > start_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df = df.limit(10) # added to speed up processings as during testing it was proving a bottleneck\n",
        "# The problem is caused by the Python UDF (reverse_geocoder) being a bottleneck due to its non-parallel nature and high computational cost per task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "def get_country_code(lat, lon):\n",
        "    \"\"\"\n",
        "    Retrieve the country code for a given latitude and longitude.\n",
        "\n",
        "    Parameters:\n",
        "    lat (float or str): Latitude of the location.\n",
        "    lon (float or str): Longitude of the location.\n",
        "\n",
        "    Returns:\n",
        "    str: Country code of the location, retrieved using the reverse geocoding API.\n",
        "\n",
        "    Example:\n",
        "    >>> get_country_details(48.8588443, 2.2943506)\n",
        "    'FR'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        coordinates = (float(lat), float(lon))\n",
        "        result = rg.search(coordinates)[0].get('cc')\n",
        "        print(f\"Processed coordinates: {coordinates} -> {result}\")\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing coordinates: {lat}, {lon} -> {str(e)}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# registering the udfs so they can be used on spark dataframes\n",
        "get_country_code_udf = udf(get_country_code, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# get_country_code(48.8588443, 2.2943506)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# adding country_code and city attributes\n",
        "df_with_location = \\\n",
        "                df.\\\n",
        "                    withColumn(\"country_code\", get_country_code_udf(col(\"latitude\"), col(\"longitude\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_with_location.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# adding significance classification\n",
        "df_with_location_sig_class = \\\n",
        "                            df_with_location.\\\n",
        "                                withColumn('sig_class', \n",
        "                                            when(col(\"sig\") < 100, \"Low\").\\\n",
        "                                            when((col(\"sig\") >= 100) & (col(\"sig\") < 500), \"Moderate\").\\\n",
        "                                            otherwise(\"High\")\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_with_location_sig_class.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Save the transformed DataFrame to the Silver container\n",
        "gold_output_path = f\"{gold_adls}earthquake_events_gold/\"\n",
        "\n",
        "# Append DataFrame to Silver container in Parquet format\n",
        "df_with_location_sig_class.write.mode('append').parquet(gold_output_path)"
      ]
    }
  ]
}