{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datetime import date, timedelta\n",
        "\n",
        "from pyspark.sql.functions import col, isnull, when\n",
        "from pyspark.sql.types import TimestampType"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the JSON string\n",
        "output_data = json.loads(bronze_output)\n",
        "\n",
        "# Access individual variables\n",
        "start_date = output_data.get(\"start_date\")\n",
        "silver_adls = output_data.get(\"silver_adls\")\n",
        "bronze_adls = output_data.get(\"bronze_adls\")\n",
        "\n",
        "# print(f\"Start Date: {start_date}\")\n",
        "# print(f\"Silver ADLS: {silver_adls}\")\n",
        "# print(f\"Bronze ADLS: {bronze_adls}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the JSON data into a Spark DataFrame\n",
        "df = spark.read.option(\"multiline\", \"true\").json(f\"{bronze_adls}{start_date}_earthquake_data.json\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape earthquake data\n",
        "df = (\n",
        "    df\n",
        "    .select(\n",
        "        'id',\n",
        "        col('geometry.coordinates').getItem(0).alias('longitude'),\n",
        "        col('geometry.coordinates').getItem(1).alias('latitude'),\n",
        "        col('geometry.coordinates').getItem(2).alias('elevation'),\n",
        "        col('properties.title').alias('title'),\n",
        "        col('properties.place').alias('place_description'),\n",
        "        col('properties.sig').alias('sig'),\n",
        "        col('properties.mag').alias('mag'),\n",
        "        col('properties.magType').alias('magType'),\n",
        "        col('properties.time').alias('time'),\n",
        "        col('properties.updated').alias('updated')\n",
        "    )\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate data: Check for missing or null values\n",
        "df = (\n",
        "    df\n",
        "    .withColumn('longitude', when(isnull(col('longitude')), 0).otherwise(col('longitude')))\n",
        "    .withColumn('latitude', when(isnull(col('latitude')), 0).otherwise(col('latitude')))\n",
        "    .withColumn('time', when(isnull(col('time')), 0).otherwise(col('time')))\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'time' and 'updated' to timestamp from Unix time\n",
        "df = (\n",
        "    df\n",
        "    .withColumn('time', (col('time') / 1000).cast(TimestampType()))\n",
        "    .withColumn('updated', (col('updated') / 1000).cast(TimestampType()))\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the transformed DataFrame to the Silver container\n",
        "silver_data = f\"{silver_adls}earthquake_events_silver/\"\n",
        "\n",
        "# Append DataFrame to Silver container in Parquet format\n",
        "df.write.mode('append').parquet(silver_data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Use mssparkutils.notebook.exit() to pass the JSON output to the pipeline\n",
        "mssparkutils.notebook.exit(silver_data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}